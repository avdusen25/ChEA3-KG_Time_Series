{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c671de6",
   "metadata": {},
   "source": [
    "# Visualizing the Enriched Transcription Factors from Time Series RNA-seq Experiments\n",
    "### The Appyter below generates a regulatory subnetwork and a UMAP visualization of the enriched TFs at different time points from time series RNA-seq data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695dcf3",
   "metadata": {},
   "source": [
    "## Loading the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "eaa55b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%appyter init\n",
    "from appyter import magic\n",
    "magic.init(lambda _=globals: _())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "aaad10ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"fe6394d2-d1b3-4f92-862a-66047c6755fb\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"fe6394d2-d1b3-4f92-862a-66047c6755fb\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"fe6394d2-d1b3-4f92-862a-66047c6755fb\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import urllib.parse\n",
    "from urllib.parse import quote\n",
    "import requests\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from IPython.display import Image as IPyImage, Javascript, display, FileLink\n",
    "import os\n",
    "import time\n",
    "\n",
    "# time series TF subnetwork visualization\n",
    "import ipycytoscape as ipc\n",
    "from ipycytoscape import CytoscapeWidget, Node, Edge\n",
    "import py4cytoscape as p4c\n",
    "import networkx as nx\n",
    "from dash import Dash, html\n",
    "\n",
    "# enriched TF UMAP visualization\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "from collections import OrderedDict\n",
    "from bokeh.io import output_notebook, export_png, export_svg\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.palettes import Category20\n",
    "import glasbey\n",
    "output_notebook()\n",
    "\n",
    "from maayanlab_bioinformatics.dge import deseq2_differential_expression\n",
    "from maayanlab_bioinformatics.dge import characteristic_direction\n",
    "from maayanlab_bioinformatics.dge import up_down_from_characteristic_direction\n",
    "from maayanlab_bioinformatics.dge import limma_voom_differential_expression\n",
    "from maayanlab_bioinformatics.dge import up_down_from_limma_voom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfe09b",
   "metadata": {},
   "source": [
    "## 1. Finding DEGs given a raw gene counts matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ceb21b",
   "metadata": {},
   "source": [
    "### Upload fields for raw gene counts matrix and experiment metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "70226435",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter hide_code\n",
    "\n",
    "{% do SectionField(\n",
    "    name= 'section1',\n",
    "    title= '1. Upload raw gene counts matrix.'\n",
    ") %}\n",
    "\n",
    "{% do SectionField(\n",
    "    name= 'section2',\n",
    "    title= '2. Upload experiment metadata.'\n",
    ") %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "aa272ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "ds1 = None\n",
       "ds1\n",
       "ds2 = None\n",
       "ds2\n",
       "compute_degs = False\n",
       "if ds1 != '' and ds2 != '':\n",
       "    compute_degs = True\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='color: red'>6: if ds1 != '' and ds2 != '':</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style='color: red'>Error: expected an indented block after 'if' statement on line 6 (<string>, line 6)</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%appyter code_eval\n",
    "\n",
    "{% set dataset1 = FileField(\n",
    "    name= 'dataset1',\n",
    "    label= 'Raw gene counts matrix',\n",
    "    default= '',\n",
    "    examples= {'Raw gene counts from TRAIL-treated TNBC cell line': 'https://drive.google.com/uc?export=download&id=1WFtUIXO2b3rbiStJ7Hf8Bx7NrfheEM9L'},\n",
    "    section= 'section1'\n",
    ") %}\n",
    "\n",
    "ds1 = {{dataset1}}\n",
    "ds1\n",
    "\n",
    "{% do DescriptionField(\n",
    "    name= 'description',\n",
    "    text= 'See example for how to format experiment metadata file. Ensure that the columns are labeled exactly as \"sample_name\" and \"time_pt_annotation\".',\n",
    "    section= 'section2'\n",
    ") %}\n",
    "\n",
    "{% set dataset2 = FileField(\n",
    "    name= 'dataset2',\n",
    "    label= 'Experiment metadata',\n",
    "    default= '',\n",
    "    examples= {'Samples taken from TRAIL-treated TNBC cell line': 'https://drive.google.com/uc?export=download&id=1iyGR_LC2MPHQ0LyobzMFULbGIYFSYH2Y'},\n",
    "    section= 'section2'\n",
    ") %}\n",
    "\n",
    "ds2 = {{dataset2}}\n",
    "ds2\n",
    "\n",
    "compute_degs = False\n",
    "if ds1 != '' and ds2 != '':\n",
    "    compute_degs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56cbfd2",
   "metadata": {},
   "source": [
    "### Method 1: PyDESeq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "de7d6b9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_degs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[518], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcompute_degs\u001b[49m:\n\u001b[1;32m      2\u001b[0m     raw_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(ds1)\n\u001b[1;32m      3\u001b[0m     sample_metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(ds2)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_degs' is not defined"
     ]
    }
   ],
   "source": [
    "if compute_degs:\n",
    "    raw_counts = pd.read_csv(ds1)\n",
    "    sample_metadata = pd.read_csv(ds2)\n",
    "\n",
    "    time_pt_list = []\n",
    "    for time_pt in sample_metadata[\"time_pt_annotation\"].tolist():\n",
    "        if time_pt not in time_pt_list:\n",
    "            time_pt_list.append(time_pt)\n",
    "\n",
    "    time_pt_dict = {}\n",
    "    for i, time_pt in enumerate(time_pt_list):\n",
    "        samples_at_time_pt = sample_metadata.loc[sample_metadata[\"time_pt_annotation\"] == time_pt, \"sample_name\"].tolist()\n",
    "        subset_counts = raw_counts[[\"gene_id\"] + samples_at_time_pt]\n",
    "        rev_subset_counts = subset_counts.set_index(\"gene_id\")\n",
    "        time_pt_dict[i] = (time_pt, rev_subset_counts)\n",
    "    # print(time_pt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cdcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [DESeq2] finding the DEGs from adjacent time pt comparisons\n",
    "if compute_degs:\n",
    "    adj_time_pt_comparisons = []\n",
    "    adj_time_pt_degs = []\n",
    "    for i in range(len(time_pt_list) - 1):\n",
    "        controls, cases = time_pt_dict[i][1], time_pt_dict[i+1][1]\n",
    "        results_df = deseq2_differential_expression(controls, cases)\n",
    "\n",
    "        p_vals = [0.05, 0.01, 0.001, 0.0001, 0.00001]\n",
    "        significant_genes = results_df[results_df[\"padj\"] < p_vals[0]]\n",
    "        up_count = (significant_genes[\"log2FoldChange\"] > 0).sum()\n",
    "        down_count = (significant_genes[\"log2FoldChange\"] < 0).sum()\n",
    "\n",
    "        idx = 1\n",
    "        while (up_count + down_count) > 2000:\n",
    "            significant_genes = results_df[results_df[\"padj\"] < p_vals[idx]]\n",
    "            up_count = (significant_genes[\"log2FoldChange\"] > 0).sum()\n",
    "            down_count = (significant_genes[\"log2FoldChange\"] < 0).sum()\n",
    "            idx += 1\n",
    "        print(\"total DEGs:\", (up_count + down_count), \"up:\", up_count, \"down:\", down_count, \"padj:\", p_vals[idx-1])\n",
    "        print(significant_genes.head())\n",
    "\n",
    "        ctrl_time_pt, case_time_pt = time_pt_dict[i][0], time_pt_dict[i+1][0]\n",
    "        adj_time_pt_comparisons.append(f\"{case_time_pt} v {ctrl_time_pt}\")\n",
    "\n",
    "        file = f\"deseq2_{case_time_pt}_v_{ctrl_time_pt}.csv\"\n",
    "        significant_genes.to_csv(file)\n",
    "        adj_time_pt_degs.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5be5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [DESeq2] finding the DEGs from time pt 0 comparisons\n",
    "if compute_degs:\n",
    "    time_pt_0_comparisons = []\n",
    "    time_pt_0_degs = []\n",
    "    for i in range(1, len(time_pt_list)):\n",
    "        controls, cases = time_pt_dict[0][1], time_pt_dict[i][1]\n",
    "        results_df = deseq2_differential_expression(controls, cases)\n",
    "\n",
    "        p_vals = [0.05, 0.01, 0.001, 0.0001, 0.00001]\n",
    "        significant_genes = results_df[results_df[\"padj\"] < p_vals[0]]\n",
    "        up_count = (significant_genes[\"log2FoldChange\"] > 0).sum()\n",
    "        down_count = (significant_genes[\"log2FoldChange\"] < 0).sum()\n",
    "\n",
    "        idx = 1\n",
    "        while (up_count + down_count) > 2000:\n",
    "            significant_genes = results_df[results_df[\"padj\"] < p_vals[idx]]\n",
    "            up_count = (significant_genes[\"log2FoldChange\"] > 0).sum()\n",
    "            down_count = (significant_genes[\"log2FoldChange\"] < 0).sum()\n",
    "            idx += 1\n",
    "        print(\"total DEGs:\", (up_count + down_count), \"up:\", up_count, \"down:\", down_count, \"padj:\", p_vals[idx-1])\n",
    "        print(significant_genes.head())\n",
    "\n",
    "        ctrl_time_pt, case_time_pt = time_pt_dict[0][0], time_pt_dict[i][0]\n",
    "        time_pt_0_comparisons.append(f\"{case_time_pt} v {ctrl_time_pt}\")\n",
    "\n",
    "        file = f\"deseq2_{case_time_pt}_v_{ctrl_time_pt}.csv\"\n",
    "        significant_genes.to_csv(file)\n",
    "        time_pt_0_degs.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a7430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_gene_list(input_csv, filename=None):\n",
    "    \"\"\"\n",
    "    Outputs list of upregulated DEGs from DESeq2 results CSV.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    row_filter = df[\"log2FoldChange\"] > 0\n",
    "    filtered = df.loc[row_filter, df.columns[0]]\n",
    "    gene_ids = list(filtered)\n",
    "\n",
    "    up_list = []\n",
    "    for gene in gene_ids:\n",
    "        if \"_\" in gene:\n",
    "            up_list.append(gene.split(\"_\", 1)[1])\n",
    "        else:\n",
    "            up_list.append(gene)\n",
    "\n",
    "    print(len(up_list))\n",
    "    return up_list\n",
    "\n",
    "\n",
    "def down_gene_list(input_csv, filename=None):\n",
    "    \"\"\"\n",
    "    Outputs list of downregulated DEGs from DESeq2 results CSV.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "    row_filter = df[\"log2FoldChange\"] < 0\n",
    "    filtered = df.loc[row_filter, df.columns[0]]\n",
    "    gene_ids = list(filtered)\n",
    "\n",
    "    down_list = []\n",
    "    for gene in gene_ids:\n",
    "        if \"_\" in gene:\n",
    "            down_list.append(gene.split(\"_\", 1)[1])\n",
    "        else:\n",
    "            down_list.append(gene)\n",
    "\n",
    "    print(len(down_list))\n",
    "    return down_list\n",
    "\n",
    "\n",
    "def csv_to_gmt(input_csv_list, comparisons, filename):\n",
    "    gmt_dict = {}\n",
    "    for i, file in enumerate(input_csv_list):\n",
    "        up_genes = up_gene_list(file)\n",
    "        print(len(up_genes))\n",
    "        down_genes = down_gene_list(file)\n",
    "        print(len(down_genes))\n",
    "        gmt_dict[f\"{comparisons[i]} up genes\"] = up_genes\n",
    "        gmt_dict[f\"{comparisons[i]} down genes\"] = down_genes\n",
    "\n",
    "    with open(filename, \"w\") as file:\n",
    "        for s,t in gmt_dict.items():\n",
    "            file.write(str(s) + \"\\t\\t\" + \"\\t\".join(t) + \"\\n\")\n",
    "    print(\"FINISHED CONVERTING TO GMT\")\n",
    "    return filename\n",
    "\n",
    "if compute_degs:\n",
    "    deseq2_degs_gmt_1 = csv_to_gmt(adj_time_pt_degs, adj_time_pt_comparisons, \"appyter_deseq2_adj_time_pt_degs.gmt\")\n",
    "    deseq2_degs_gmt_2 = csv_to_gmt(time_pt_0_degs, time_pt_0_comparisons, \"appyter_deseq2_compare_w_time_pt_0_degs.gmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b27a30",
   "metadata": {},
   "source": [
    "### Method 2: Characteristic Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### [CD] finding the DEGs from adjacent time pt comparisons\n",
    "def run_chea_kg(gene_list, num_tfs):\n",
    "    \"\"\"\n",
    "    Outputs JSON of TF subnetwork best corresponding to input gene list.\n",
    "    \"\"\"\n",
    "    CHEA_KG = 'https://chea-kg.maayanlab.cloud/api/enrichment'\n",
    "\n",
    "    description = \"insert description here\"\n",
    "    payload = {\n",
    "        'list': (None, \"\\n\".join(gene_list)),\n",
    "        'description': (None, description)\n",
    "    }\n",
    "    response=requests.post(f\"{CHEA_KG}/addList\", files=payload)\n",
    "    time.sleep(0.2)\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    q = {\n",
    "        'min_lib': 3, # minimum number of libraries that a TF must be ranked in\n",
    "        'libraries': [\n",
    "            {'library': \"Integrated--meanRank\", 'term_limit': num_tfs} # edit term_limit to change number of top-ranked TFs\n",
    "        ],\n",
    "        'limit':50, # controls number of edges returned - may cause issues with visualization if too large\n",
    "        'userListId': data['userListId']\n",
    "    }\n",
    "    query_json=json.dumps(q)\n",
    "    res = requests.post(CHEA_KG, data=query_json)\n",
    "    if res.ok:\n",
    "        data = json.loads(res.text)\n",
    "        return data\n",
    "    else:\n",
    "        data = None\n",
    "        return res.text\n",
    "\n",
    "\n",
    "def top_tfs(gene_list, num_tfs=5):\n",
    "    \"\"\"\n",
    "    Returns a list of the top N most enriched TFs corresponding to an input gene list.\n",
    "    \"\"\"\n",
    "    enriched_tfs = run_chea_kg(gene_list, num_tfs)\n",
    "    tfs_list = []\n",
    "    for node in enriched_tfs[\"nodes\"]:\n",
    "        tfs_list.append(node[\"data\"][\"label\"])\n",
    "    return tfs_list\n",
    "\n",
    "if compute_degs:\n",
    "    cd_adj_time_pt_comparisons = []\n",
    "    cd_tf_time_dict_1 = {}\n",
    "    for i in range(len(time_pt_list) - 1):\n",
    "        ctrl_time_pt, case_time_pt = time_pt_dict[i][0], time_pt_dict[i+1][0]\n",
    "        cd_adj_time_pt_comparisons.append(f\"{case_time_pt} v {ctrl_time_pt}\")\n",
    "\n",
    "        controls, cases = time_pt_dict[i][1], time_pt_dict[i+1][1]\n",
    "        results_df = characteristic_direction(controls, cases)\n",
    "        # print(results_df.head())\n",
    "\n",
    "        up_genes = up_down_from_characteristic_direction(results_df).up\n",
    "        up_list = []\n",
    "        for gene in up_genes:\n",
    "            if \"_\" in gene:\n",
    "                up_list.append(gene.split(\"_\", 1)[1])\n",
    "            else:\n",
    "                up_list.append(gene)\n",
    "\n",
    "        down_genes = up_down_from_characteristic_direction(results_df).down\n",
    "        down_list = []\n",
    "        for gene in down_genes:\n",
    "            if \"_\" in gene:\n",
    "                down_list.append(gene.split(\"_\", 1)[1])\n",
    "            else:\n",
    "                down_list.append(gene)\n",
    "\n",
    "        # print(up_list)\n",
    "        # print(down_list)\n",
    "        print(len(up_list), len(down_list))\n",
    "\n",
    "        cd_tf_time_dict_1[i] = (top_tfs(up_list), top_tfs(down_list))\n",
    "    print(cd_tf_time_dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86251ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_degs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[451], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### [CD] degs from time pt 0 comparions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcompute_degs\u001b[49m:\n\u001b[1;32m      3\u001b[0m     cd_time_pt_0_comparisons \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     cd_tf_time_dict_2 \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_degs' is not defined"
     ]
    }
   ],
   "source": [
    "### [CD] finding the DEGs from time pt 0 comparisons\n",
    "if compute_degs:\n",
    "    cd_time_pt_0_comparisons = []\n",
    "    cd_tf_time_dict_2 = {}\n",
    "    for i in range(1, len(time_pt_list)):\n",
    "        ctrl_time_pt, case_time_pt = time_pt_dict[0][0], time_pt_dict[i][0]\n",
    "        cd_time_pt_0_comparisons.append(f\"{case_time_pt} v {ctrl_time_pt}\")\n",
    "\n",
    "        controls, cases = time_pt_dict[0][1], time_pt_dict[i][1]\n",
    "        results_df = characteristic_direction(controls, cases)\n",
    "\n",
    "        up_genes = up_down_from_characteristic_direction(results_df).up\n",
    "        up_list = []\n",
    "        for gene in up_genes:\n",
    "            if \"_\" in gene:\n",
    "                up_list.append(gene.split(\"_\", 1)[1])\n",
    "            else:\n",
    "                up_list.append(gene)\n",
    "\n",
    "        down_genes = up_down_from_characteristic_direction(results_df).down\n",
    "        down_list = []\n",
    "        for gene in down_genes:\n",
    "            if \"_\" in gene:\n",
    "                down_list.append(gene.split(\"_\", 1)[1])\n",
    "            else:\n",
    "                down_list.append(gene)\n",
    "\n",
    "        # print(up_list)\n",
    "        # print(down_list)\n",
    "        print(len(up_list), len(down_list))\n",
    "\n",
    "        cd_tf_time_dict_2[i-1] = (top_tfs(up_list), top_tfs(down_list))\n",
    "    print(cd_tf_time_dict_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5424dc0",
   "metadata": {},
   "source": [
    "## 2. Defining functions that output the enriched TFs given an input gene set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0175641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chea_kg(gene_list, num_tfs):\n",
    "    \"\"\"\n",
    "    Outputs JSON of TF subnetwork best corresponding to input gene list.\n",
    "    \"\"\"\n",
    "    CHEA_KG = 'https://chea-kg.maayanlab.cloud/api/enrichment'\n",
    "\n",
    "    description = \"insert description here\"\n",
    "    payload = {\n",
    "        'list': (None, \"\\n\".join(gene_list)),\n",
    "        'description': (None, description)\n",
    "    }\n",
    "    response=requests.post(f\"{CHEA_KG}/addList\", files=payload)\n",
    "    time.sleep(0.2)\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    q = {\n",
    "        'min_lib': 3, # minimum number of libraries that a TF must be ranked in\n",
    "        'libraries': [\n",
    "            {'library': \"Integrated--meanRank\", 'term_limit': num_tfs} # edit term_limit to change number of top-ranked TFs\n",
    "        ],\n",
    "        'limit':50, # controls number of edges returned - may cause issues with visualization if too large\n",
    "        'userListId': data['userListId']\n",
    "    }\n",
    "    query_json=json.dumps(q)\n",
    "\n",
    "    res = requests.post(CHEA_KG, data=query_json)\n",
    "    if res.ok:\n",
    "        data = json.loads(res.text)\n",
    "        return data\n",
    "    else:\n",
    "        data = None\n",
    "        return res.text\n",
    "\n",
    "\n",
    "def top_tfs(gene_list, num_tfs=5):\n",
    "    \"\"\"\n",
    "    Returns a list of the top N most enriched TFs corresponding to an input gene list.\n",
    "    \"\"\"\n",
    "    enriched_tfs = run_chea_kg(gene_list, num_tfs)\n",
    "    tfs_list = []\n",
    "    for node in enriched_tfs[\"nodes\"]:\n",
    "        tfs_list.append(node[\"data\"][\"label\"])\n",
    "    return tfs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa8d98",
   "metadata": {},
   "source": [
    "## 3. Defining subnetwork visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_chea_kg_data(start_tf, end_tf):\n",
    "    \"\"\"\n",
    "    Outputs JSON data for shortest path connecting two TFs.\n",
    "    \"\"\"\n",
    "    base_url = \"https://chea-kg.maayanlab.cloud/api/knowledge_graph\"\n",
    "\n",
    "    query_filter = {\n",
    "        \"start\": \"Transcription Factor\",\n",
    "        \"start_field\": \"label\",\n",
    "        \"start_term\": start_tf,\n",
    "        \"end\": \"Transcription Factor\",\n",
    "        \"end_field\": \"label\",\n",
    "        \"end_term\": end_tf\n",
    "    }\n",
    "\n",
    "    encoded_filter = urllib.parse.quote(str(query_filter).replace(\"'\", '\"'))\n",
    "    full_url = f\"{base_url}?filter={encoded_filter}\"\n",
    "\n",
    "    response = requests.get(full_url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_tf_node_info(tf_label):\n",
    "    \"\"\"\n",
    "    Gets node information associated with single TF.\n",
    "    \"\"\"\n",
    "    base_url = \"https://chea-kg.maayanlab.cloud/api/knowledge_graph\"\n",
    "\n",
    "    query_filter = {\n",
    "        \"start\": \"Transcription Factor\",\n",
    "        \"start_field\": \"label\",\n",
    "        \"start_term\": tf_label\n",
    "    }\n",
    "    encoded_filter = urllib.parse.quote(str(query_filter).replace(\"'\", '\"'))\n",
    "    full_url = f\"{base_url}?filter={encoded_filter}\"\n",
    "\n",
    "    response = requests.get(full_url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    for node in data[\"nodes\"]:\n",
    "        if node[\"data\"][\"label\"] == tf_label:\n",
    "            return node\n",
    "    raise ValueError(f\"Node for TF '{tf_label}' not found in response.\")\n",
    "\n",
    "\n",
    "def get_tf_edge_info(tf_label):\n",
    "    \"\"\"\n",
    "    Returns edge info if TF is autoregulatory.\n",
    "    \"\"\"\n",
    "    base_url = \"https://chea-kg.maayanlab.cloud/api/knowledge_graph\"\n",
    "\n",
    "    query_filter = {\n",
    "        \"start\": \"Transcription Factor\",\n",
    "        \"start_field\": \"label\",\n",
    "        \"start_term\": tf_label\n",
    "    }\n",
    "    encoded_filter = urllib.parse.quote(str(query_filter).replace(\"'\", '\"'))\n",
    "    full_url = f\"{base_url}?filter={encoded_filter}\"\n",
    "\n",
    "    response = requests.get(full_url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    for edge in data[\"edges\"]:\n",
    "        if edge[\"data\"][\"source_label\"] == tf_label and edge[\"data\"][\"target_label\"] == tf_label:\n",
    "            return edge\n",
    "    raise ValueError(f\"Self-edge for TF '{tf_label}' not found in response.\")\n",
    "\n",
    "\n",
    "def create_tf_time_series_graph(tf_time_dict, num_comparisons, filename):\n",
    "    \"\"\"\n",
    "    Creates network of TFs given differentially expressed genes at each time point.\n",
    "    \"\"\"\n",
    "    subnetwork = {\"nodes\": [], \"edges\": []}\n",
    "    for time in tf_time_dict.keys():\n",
    "        up_tfs = tf_time_dict[time][0]\n",
    "        down_tfs = tf_time_dict[time][1]\n",
    "\n",
    "        for tf in up_tfs:\n",
    "            node_info = get_tf_node_info(tf)\n",
    "            node_info[\"data\"][\"color\"] = \"#80eaff\"\n",
    "            node_info[\"data\"][\"id\"] = f\"{node_info['data']['label']}_up_{time}\"\n",
    "            subnetwork[\"nodes\"].append(node_info)\n",
    "\n",
    "        for tf in down_tfs:\n",
    "            node_info = get_tf_node_info(tf)\n",
    "            node_info[\"data\"][\"color\"] = \"#ff8a80\"\n",
    "            node_info[\"data\"][\"id\"] = f\"{node_info['data']['label']}_down_{time}\"\n",
    "            subnetwork[\"nodes\"].append(node_info)\n",
    "\n",
    "    for i in range(num_comparisons-1):\n",
    "        for j, source_tf_list in enumerate(tf_time_dict[i]):\n",
    "            for k, target_tf_list in enumerate(tf_time_dict[i+1]):\n",
    "                for source_tf in source_tf_list:\n",
    "                    for target_tf in target_tf_list:\n",
    "                        if source_tf != target_tf:\n",
    "                            data = fetch_chea_kg_data(source_tf, target_tf)\n",
    "                            if len(data[\"nodes\"]) == 2 and len(data[\"edges\"]) == 1:\n",
    "                                if data[\"edges\"][0][\"data\"][\"source_label\"] == source_tf and \\\n",
    "                                    data[\"edges\"][0][\"data\"][\"target_label\"] == target_tf:\n",
    "                                    if j == 0 and k == 0:\n",
    "                                        data[\"edges\"][0][\"data\"][\"source\"] = f\"{source_tf}_up_{i}\"\n",
    "                                        data[\"edges\"][0][\"data\"][\"target\"] = f\"{target_tf}_up_{i+1}\"\n",
    "                                    elif j == 0 and k == 1:\n",
    "                                        data[\"edges\"][0][\"data\"][\"source\"] = f\"{source_tf}_up_{i}\"\n",
    "                                        data[\"edges\"][0][\"data\"][\"target\"] = f\"{target_tf}_down_{i+1}\"\n",
    "                                    elif j == 1 and k == 0:\n",
    "                                        data[\"edges\"][0][\"data\"][\"source\"] = f\"{source_tf}_down_{i}\"\n",
    "                                        data[\"edges\"][0][\"data\"][\"target\"] = f\"{target_tf}_up_{i+1}\"\n",
    "                                    elif j == 1 and k == 1:\n",
    "                                        data[\"edges\"][0][\"data\"][\"source\"] = f\"{source_tf}_down_{i}\"\n",
    "                                        data[\"edges\"][0][\"data\"][\"target\"] = f\"{target_tf}_down_{i+1}\"\n",
    "                                    subnetwork[\"edges\"].append(data[\"edges\"][0])\n",
    "                                    print(\"edge added, case 1\")\n",
    "                        else:\n",
    "                            if len(source_tf_list) != 0:\n",
    "                                try:\n",
    "                                    edge = get_tf_edge_info(source_tf)\n",
    "                                    if j == 0 and k == 0:\n",
    "                                        edge[\"data\"][\"source\"] = f\"{source_tf}_up_{i}\"\n",
    "                                        edge[\"data\"][\"target\"] = f\"{target_tf}_up_{i+1}\"\n",
    "                                    elif j == 0 and k == 1:\n",
    "                                        edge[\"data\"][\"source\"] = f\"{source_tf}_up_{i}\"\n",
    "                                        edge[\"data\"][\"target\"] = f\"{target_tf}_down_{i+1}\"\n",
    "                                    elif j == 1 and k == 0:\n",
    "                                        edge[\"data\"][\"source\"] = f\"{source_tf}_down_{i}\"\n",
    "                                        edge[\"data\"][\"target\"] = f\"{target_tf}_up_{i+1}\"\n",
    "                                    elif j == 1 and k == 1:\n",
    "                                        edge[\"data\"][\"source\"] = f\"{source_tf}_down_{i}\"\n",
    "                                        edge[\"data\"][\"target\"] = f\"{target_tf}_down_{i+1}\"\n",
    "                                    subnetwork[\"edges\"].append(edge)\n",
    "                                    print(\"edge added, case 2\")\n",
    "                                except ValueError:\n",
    "                                    continue\n",
    "    output_path = f\"{filename}.json\"\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        json.dump(subnetwork, outfile, indent=4)\n",
    "    print(\"FINISHED\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def gmt_to_tf_time_dict(gmt_file):\n",
    "    \"\"\"\n",
    "    Converts GMT file containing DEGs to tf_time_dict.\n",
    "    \"\"\"\n",
    "    with open(gmt_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    temp_dict = {}\n",
    "    for line in lines:\n",
    "        tokens = line.split(\"\\t\\t\")\n",
    "        term = tokens[0]\n",
    "        genes = [x.split(',')[0].strip() for x in tokens[1].split('\\t')]\n",
    "        temp_dict[term] = top_tfs(genes)\n",
    "        print(\"enriched TFs found\")\n",
    "\n",
    "    comparisons = list(temp_dict.keys())\n",
    "    new_comparisons = []\n",
    "    for item in comparisons:\n",
    "        comp = item.rsplit(' ', 2)[0]\n",
    "        if comp not in new_comparisons:\n",
    "            new_comparisons.append(comp)\n",
    "    print(\"Time point comparisons:\", new_comparisons)\n",
    "\n",
    "    j = 0\n",
    "    tf_time_dict = {}\n",
    "    for i in range(len(comparisons) // 2):\n",
    "        tf_time_dict[i] = (temp_dict[comparisons[j]], temp_dict[comparisons[j+1]])\n",
    "        j += 2\n",
    "    print(tf_time_dict)\n",
    "    return tf_time_dict, new_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter hide_code\n",
    "\n",
    "{% do SectionField(\n",
    "    name= 'data',\n",
    "    title= '3. Upload differentially expressed genes (only if uploading a GMT file rather than a raw gene counts matrix).'\n",
    ")%}\n",
    "\n",
    "{% do SectionField(\n",
    "    name= 'section4',\n",
    "    title= '4. Provide a short description of your study (optional).'\n",
    ") %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693db24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "ds1 = None\n",
       "ds1\n",
       "ds2 = None\n",
       "ds2\n",
       "umap_desc = ''''''\n",
       "umap_desc\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%appyter code_eval\n",
    "\n",
    "{% do DescriptionField(\n",
    "    name= 'description_2',\n",
    "    text= 'See example files for how to format input GMT file. Each row should consist of either the \"up\" or \"down\" differentially expressed genes at each time point comparison.',\n",
    "    section= 'data'\n",
    ") %}\n",
    "\n",
    "{% set dataset3 = FileField(\n",
    "    name= 'dataset3',\n",
    "    label= 'GMT file containing DEGs from ADJACENT TIME POINT COMPARISONS',\n",
    "    default= '',\n",
    "    examples= {'First set of DEGs from TRAIL-treated TNBC cell line': 'https://drive.google.com/uc?export=download&id=1tFNWDYPX553R1g9RDIUfMkoY2j3lXEw4'},\n",
    "    section= 'data'\n",
    ") %}\n",
    "\n",
    "ds3 = {{dataset3}}\n",
    "ds3\n",
    "\n",
    "{% set dataset4 = FileField(\n",
    "    name= 'dataset4',\n",
    "    label= 'GMT file containing DEGs from COMPARISONS WITH TIME POINT 0',\n",
    "    default= '',\n",
    "    examples= {'Second set of DEGS from TRAIL-treated TNBC cell line': 'https://drive.google.com/uc?export=download&id=1NSFdnybH35EJhXY6_iHZf09PS-EeFvQh'},\n",
    "    section= 'data'\n",
    ") %}\n",
    "\n",
    "ds4 = {{dataset4}}\n",
    "ds4\n",
    "\n",
    "{% set user_description = TextField(\n",
    "        name= 'user_description',\n",
    "        label= 'Description',\n",
    "        default= '',\n",
    "        section = 'section4',\n",
    ") %}\n",
    "\n",
    "umap_desc = {{user_description}}\n",
    "umap_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde58a65",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[419], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_time_dict_1, comparisons_1 \u001b[38;5;241m=\u001b[39m \u001b[43mgmt_to_tf_time_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m subnetwork_data_1 \u001b[38;5;241m=\u001b[39m create_tf_time_series_graph(tf_time_dict_1, \u001b[38;5;28mlen\u001b[39m(comparisons_1), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappyter_deseq2_adj_time_pts_top_5_tfs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m tf_time_dict_2, comparisons_2 \u001b[38;5;241m=\u001b[39m gmt_to_tf_time_dict(ds2)\n",
      "Cell \u001b[0;32mIn[416], line 148\u001b[0m, in \u001b[0;36mgmt_to_tf_time_dict\u001b[0;34m(gmt_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgmt_to_tf_time_dict\u001b[39m(gmt_file):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    Converts GMT file containing DEGs to tf_time_dict.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgmt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    149\u001b[0m         lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m    151\u001b[0m     temp_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Downloads/mount_sinai_internship/appyter_310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "if compute_degs:\n",
    "    deseq2_tf_time_dict_1, deseq2_adj_time_pt_comparisons = gmt_to_tf_time_dict(deseq2_degs_gmt_1)\n",
    "    deseq2_subnetwork_data_1 = create_tf_time_series_graph(deseq2_tf_time_dict_1, len(deseq2_adj_time_pt_comparisons), \"appyter_deseq2_adj_time_pts_top_5_tfs\")\n",
    "\n",
    "    deseq2_tf_time_dict_2, deseq2_time_pt_0_comparisons = gmt_to_tf_time_dict(deseq2_degs_gmt_2)\n",
    "    deseq2_subnetwork_data_2 = create_tf_time_series_graph(deseq2_tf_time_dict_2, len(deseq2_time_pt_0_comparisons), \"appyter_deseq2_compare_w_time_pt_0_top_5_tfs\")\n",
    "\n",
    "    cd_subnetwork_data_1 = create_tf_time_series_graph(cd_tf_time_dict_1, len(cd_adj_time_pt_comparisons), \"appyter_cd_adj_time_pts_top_5_tfs\")\n",
    "    cd_subnetwork_data_2 = create_tf_time_series_graph(cd_tf_time_dict_2, len(cd_time_pt_0_comparisons), \"appyter_cd_compare_w_time_pt_0_top_5_tfs\")\n",
    "\n",
    "else:\n",
    "    tf_time_dict_1, comparisons_1 = gmt_to_tf_time_dict(ds3)\n",
    "    subnetwork_data_1 = create_tf_time_series_graph(tf_time_dict_1, len(comparisons_1), \"appyter_deseq2_adj_time_pts_top_5_tfs\")\n",
    "\n",
    "    tf_time_dict_2, comparisons_2 = gmt_to_tf_time_dict(ds4)\n",
    "    subnetwork_data_2 = create_tf_time_series_graph(tf_time_dict_2, len(comparisons_2), \"appyter_deseq2_compare_w_time_pt_0_top_5_tfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3140530",
   "metadata": {},
   "source": [
    "## 4. Visualizing the enriched TFs within a time series regulatory subnetwork\n",
    "### Blue and red nodes correspond to TFs enriched for upregulated and downregulated gene sets, respectively, at the given time point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_cytoscape_json(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    display(FileLink(filename))\n",
    "\n",
    "\n",
    "def visualize_network_2(data, filename):\n",
    "    row_col_dict = {}\n",
    "    for node in data[\"nodes\"]:\n",
    "        if \"position\" in node:\n",
    "            continue\n",
    "        node_id = node[\"data\"][\"id\"]\n",
    "        row_index = int(node_id.split(\"_\")[-1])\n",
    "        col_index = row_col_dict.get(row_index, 1)\n",
    "        row_col_dict[row_index] = col_index + 1\n",
    "        node[\"position\"] = {\"x\": col_index * 150, \"y\": row_index * 150}\n",
    "\n",
    "    cyto_widget = CytoscapeWidget()\n",
    "    cyto_widget.graph.add_graph_from_json(data)\n",
    "    cyto_widget.set_layout(name='preset')\n",
    "\n",
    "    cyto_widget.set_style([{\n",
    "        'selector': 'node',\n",
    "        'style': {\n",
    "            'label': 'data(label)',\n",
    "            'background-color': 'data(color)',\n",
    "            'border-width': 'data(borderWidth)',\n",
    "            'border-color': 'data(borderColor)',\n",
    "            'width': 40,\n",
    "            'height': 40\n",
    "        }\n",
    "    }, {\n",
    "        'selector': '.row-label',\n",
    "        'style': {\n",
    "            'label': 'data(label)',\n",
    "            'background-color': '#ffffff',\n",
    "            'color': '#000000',\n",
    "            'font-size': '20px',\n",
    "            'width': 5,\n",
    "            'height': 5,\n",
    "            'text-valign': 'center',\n",
    "            'text-halign': 'center'\n",
    "        }\n",
    "    }, {\n",
    "        'selector': 'edge',\n",
    "        'style': {\n",
    "            'width': 3,\n",
    "            'line-color': 'data(lineColor)',\n",
    "            'target-arrow-color': 'data(lineColor)',\n",
    "            'target-arrow-shape': 'data(directed)',\n",
    "            'curve-style': 'bezier',\n",
    "            'arrow-scale': 1.5\n",
    "        }\n",
    "    }])\n",
    "\n",
    "    display(cyto_widget)\n",
    "    json_filename = f\"{filename}.json\"\n",
    "    export_cytoscape_json(data, json_filename)\n",
    "\n",
    "\n",
    "def run_visualization(input_json, comparisons, filename):\n",
    "    if isinstance(input_json, dict):\n",
    "        data = input_json\n",
    "    else:\n",
    "        with open(input_json, \"r\") as input_file:\n",
    "            data = json.load(input_file)\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    for i, comparison in enumerate(comparisons):\n",
    "        label_node = {\n",
    "            'data': {\n",
    "                'id': f'row_label_{i}',\n",
    "                'label': comparison,\n",
    "                'color': '#ffffff',\n",
    "                'borderColor': '#ffffff',\n",
    "                'borderWidth': 0\n",
    "            },\n",
    "            'classes': 'row-label',\n",
    "            'position': {\"x\": 0, \"y\": i * 150}\n",
    "        }\n",
    "        nodes.append(label_node)\n",
    "\n",
    "    visualize_network_2(data, filename)\n",
    "    return \"VISUALIZATION HAS RUN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc8b00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subnetwork_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m comparisons \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHour 1 vs Hour 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHour 3 vs Hour 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHour 6 vs Hour 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHour 12 vs Hour 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHour 24 vs Hour 0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m input_json \u001b[38;5;241m=\u001b[39m \u001b[43msubnetwork_data\u001b[49m\n\u001b[1;32m      3\u001b[0m run_visualization(input_json, comparisons)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subnetwork_data' is not defined"
     ]
    }
   ],
   "source": [
    "# if compute_degs:\n",
    "#     print(\"deseq2_adj_time_pt_visualization:\")\n",
    "#     run_visualization(deseq2_subnetwork_data_1, deseq2_adj_time_pt_comparisons, \"deseq2_adj_time_pt_visualization\")\n",
    "#     print(\"deseq2_compare_w_time_pt_0_visualization:\")\n",
    "#     run_visualization(deseq2_subnetwork_data_2, deseq2_time_pt_0_comparisons, \"deseq2_compare_w_time_pt_0_visualization\")\n",
    "\n",
    "#     print(\"cd_adj_time_pt_visualization:\")\n",
    "#     run_visualization(cd_subnetwork_data_1, cd_adj_time_pt_comparisons, \"cd_adj_time_pt_visualization\")\n",
    "#     print(\"cd_compare_w_time_pt_0_visualization:\")\n",
    "#     run_visualization(cd_subnetwork_data_2, cd_time_pt_0_comparisons, \"cd_compare_w_time_pt_0_visualization\")\n",
    "# else:\n",
    "#     input_json_1 = subnetwork_data_1\n",
    "#     run_visualization(input_json_1, comparisons_1, \"deseq2_adj_time_pt_visualization\")\n",
    "\n",
    "#     input_json_2 = subnetwork_data_2\n",
    "#     run_visualization(input_json_2, comparisons_2, \"deseq2_compare_w_time_pt_0_visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_degs:\n",
    "    print(\"deseq2_adj_time_pt_visualization:\")\n",
    "    run_visualization(deseq2_subnetwork_data_1, deseq2_adj_time_pt_comparisons, \"deseq2_adj_time_pt_visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_degs:\n",
    "    print(\"deseq2_compare_w_time_pt_0_visualization:\")\n",
    "    run_visualization(deseq2_subnetwork_data_2, deseq2_time_pt_0_comparisons, \"deseq2_compare_w_time_pt_0_visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_degs:\n",
    "    print(\"cd_adj_time_pt_visualization:\")\n",
    "    run_visualization(cd_subnetwork_data_1, cd_adj_time_pt_comparisons, \"cd_adj_time_pt_visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375df57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_degs:\n",
    "    print(\"cd_compare_w_time_pt_0_visualization:\")\n",
    "    run_visualization(cd_subnetwork_data_2, cd_time_pt_0_comparisons, \"cd_compare_w_time_pt_0_visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64243b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not compute_degs:\n",
    "    input_json_1 = subnetwork_data_1\n",
    "    run_visualization(input_json_1, comparisons_1, \"deseq2_adj_time_pt_visualization\")\n",
    "\n",
    "    input_json_2 = subnetwork_data_2\n",
    "    run_visualization(input_json_2, comparisons_2, \"deseq2_compare_w_time_pt_0_visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c947143",
   "metadata": {},
   "source": [
    "## 5. Generating a UMAP visualization for enriched TFs\n",
    "\n",
    "### The enriched transcription factors from each time point are colored on a UMAP plot of 700 TFs identified by ChEA to be \"source TFs\" (i.e. they exert regulatory effects on other TFs). The UMAP algorithm was performed using the TF-IDF scores of the TFs' target genes, meaning that TFs placed in the same cluster generally regulate similar genes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90965bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scatterplot(libdict, nneighbors=30, mindist=0.1, spread=1.0, maxdf=1.0, mindf=1):\n",
    "    print(\"\\tTF-IDF vectorizing gene set data...\")\n",
    "    vec = TfidfVectorizer(max_df=maxdf, min_df=mindf)\n",
    "    X = vec.fit_transform(libdict.values())\n",
    "    print(X.shape)\n",
    "    adata = anndata.AnnData(X)\n",
    "    adata.obs.index = libdict.keys()\n",
    "\n",
    "    print(\"\\tPerforming Leiden clustering...\")\n",
    "    ### the nneighbors and min_dist parameters can be altered\n",
    "    sc.pp.neighbors(adata, n_neighbors=nneighbors)\n",
    "    sc.tl.leiden(adata, resolution=1.0)\n",
    "    sc.tl.umap(adata, min_dist=mindist, spread=spread, random_state=42)\n",
    "\n",
    "    new_order = adata.obs.sort_values(by='leiden').index.tolist()\n",
    "    adata = adata[new_order, :]\n",
    "    adata.obs['leiden'] = 'Cluster ' + adata.obs['leiden'].astype('object')\n",
    "\n",
    "    df = pd.DataFrame(adata.obsm['X_umap'])\n",
    "    df.columns = ['x', 'y']\n",
    "\n",
    "    df['cluster'] = adata.obs['leiden'].values\n",
    "    df['term'] = adata.obs.index\n",
    "    df['genes'] = [libdict[l] for l in df['term']]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_scatter_colors(df):\n",
    "    clusters = pd.unique(df['cluster']).tolist()\n",
    "    n_clusters = len(clusters)\n",
    "    gray_shades = [f'#{int(v):02x}{int(v):02x}{int(v):02x}' for v in np.linspace(50, 230, n_clusters)]\n",
    "    color_mapper = {clusters[i]: gray_shades[i] for i in range(n_clusters)}\n",
    "    return color_mapper\n",
    "\n",
    "\n",
    "def generate_df_for_comparison(base_df, tf_pair, comparison_label, comparison_idx):\n",
    "    \"\"\"\n",
    "    Generates a new df for each time point.\n",
    "    \"\"\"\n",
    "    up_tfs, down_tfs = tf_pair[0], tf_pair[1]\n",
    "    df = base_df.copy()\n",
    "    color_mapper = get_scatter_colors(df)\n",
    "    df['color'] = df['cluster'].apply(lambda x: color_mapper[x])\n",
    "    df['size'] = 6\n",
    "    df['time_point'] = \"Not enriched\"\n",
    "\n",
    "    for idx, term in df['term'].items():\n",
    "        if (term in up_tfs) and (term not in down_tfs):\n",
    "            df.at[idx, 'color'] = \"#1595f0\"\n",
    "            df.at[idx, 'size'] = 12\n",
    "            df.at[idx, 'time_point'] = comparison_label\n",
    "        if (term in down_tfs) and (term not in up_tfs):\n",
    "            df.at[idx, 'color'] = \"#f30a1a\"\n",
    "            df.at[idx, 'size'] = 12\n",
    "            df.at[idx, 'time_point'] = comparison_label\n",
    "        if (term in up_tfs) and (term in down_tfs):\n",
    "            df.at[idx, 'color'] = \"#26e411\"\n",
    "            df.at[idx, 'size'] = 12\n",
    "            df.at[idx, 'time_point'] = comparison_label\n",
    "    return df\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io.export import export_png\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Slider, CustomJS, Title, Label\n",
    "from bokeh.layouts import column\n",
    "from bokeh.palettes import Greys\n",
    "from bokeh.io import show\n",
    "from bokeh.plotting import output_file, save\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_scatterplot(scatterdf, tf_time_dict=None, comparisons=None, legend_description=None, image_dir=None, gif_filename=None):\n",
    "    \"\"\"\n",
    "    Generates images navigable via a slider, as well as all the images separately.\n",
    "    \"\"\"\n",
    "    df = scatterdf.copy()\n",
    "    df['cluster_number'] = df['cluster'].apply(lambda x: int(x.split(\" \")[-1]))\n",
    "    print(df['cluster_number'])\n",
    "    df.sort_values(by=['cluster_number'], inplace=True)\n",
    "    df.drop(columns = ['cluster_number'], inplace=True)\n",
    "\n",
    "    sources = []\n",
    "    for i, label in enumerate(comparisons):\n",
    "        df_comp = generate_df_for_comparison(df, tf_time_dict[i], label, i)\n",
    "        source = ColumnDataSource(data=dict(x = df_comp['x'], y = df_comp['y'],\n",
    "                                            gene_set = df_comp['term'], colors = df_comp['color'],\n",
    "                                            label = df_comp['cluster'], size = df_comp['size'],\n",
    "                                            time_point = df_comp['time_point']))\n",
    "        sources.append(source)\n",
    "\n",
    "    source = sources[0]\n",
    "    tooltips = [\n",
    "        (\"Gene Set\", \"@gene_set\"),\n",
    "        (\"Cluster\", \"@label\"),\n",
    "        (\"Time point\", \"@time_point\")\n",
    "    ]\n",
    "\n",
    "    hover_emb = HoverTool(tooltips=tooltips)\n",
    "    tools_emb = [hover_emb, 'pan', 'wheel_zoom', 'reset', 'save']\n",
    "\n",
    "    plot_emb = figure(\n",
    "        width=500*2,\n",
    "        height=400*2,\n",
    "        tools=tools_emb,\n",
    "        output_backend='canvas'\n",
    "    )\n",
    "\n",
    "    plot_emb.scatter(\n",
    "        'x',\n",
    "        'y',\n",
    "        size = 'size',\n",
    "        source = source,\n",
    "        marker='circle',\n",
    "        fill_color = 'colors',\n",
    "        color='colors',\n",
    "        legend_group = 'label',\n",
    "    )\n",
    "\n",
    "    # hide axis labels and grid lines\n",
    "    plot_title = Title(text=comparisons[0], align='center')\n",
    "    plot_title.text_font_size = '20pt'\n",
    "    plot_title.text_font_style = 'bold'\n",
    "    plot_emb.add_layout(plot_title, 'above')\n",
    "\n",
    "    plot_emb.xaxis.major_tick_line_color = None\n",
    "    plot_emb.xaxis.minor_tick_line_color = None\n",
    "    plot_emb.yaxis.major_tick_line_color = None\n",
    "    plot_emb.yaxis.minor_tick_line_color = None\n",
    "    plot_emb.grid.grid_line_color = None\n",
    "    plot_emb.xaxis.major_label_text_font_size = '0pt'\n",
    "    plot_emb.yaxis.major_label_text_font_size = '0pt'\n",
    "\n",
    "    plot_emb.xaxis.axis_label = \"UMAP-1\"\n",
    "    plot_emb.yaxis.axis_label = \"UMAP-2\"\n",
    "    plot_emb.xaxis.axis_label_text_font_size = '20pt'\n",
    "    plot_emb.yaxis.axis_label_text_font_size = '20pt'\n",
    "    plot_emb.xaxis.axis_label_text_font_style = \"normal\"\n",
    "    plot_emb.yaxis.axis_label_text_font_style = \"normal\"\n",
    "\n",
    "    plot_emb.legend.label_text_font_size = '18pt'\n",
    "    plot_emb.legend.glyph_height = 20\n",
    "    plot_emb.legend.glyph_width = 20\n",
    "\n",
    "    print(\"legend\", plot_emb.legend[0])\n",
    "    plot_emb.add_layout(plot_emb.legend[0], 'right')\n",
    "\n",
    "    plot_emb.min_border_bottom = 168\n",
    "\n",
    "    description_label = Label(x=0, y=-7, x_units='screen', y_units='screen',\n",
    "                          text=legend_description,\n",
    "                          text_font_size='12pt', text_align='left')\n",
    "\n",
    "    plot_emb.add_layout(description_label, 'below')\n",
    "\n",
    "    ### adding a slider ###\n",
    "    slider = Slider(start=0, end=len(sources) - 1, value=0, step=1, title=\"Comparison\")\n",
    "    comparison_source = ColumnDataSource(data=dict(comparisons=[str(c) for c in comparisons]))\n",
    "    callback = CustomJS(args=dict(source=source, slider=slider, sources=sources, plot=plot_emb,\n",
    "                                  comparison_source=comparison_source, title_obj=plot_title), code=\"\"\"\n",
    "        const i = slider.value;\n",
    "        const new_data = sources[i].data;\n",
    "        const copied_data = {};\n",
    "        for (const key in new_data) {\n",
    "            copied_data[key] = [...new_data[key]];  // deep copy each column\n",
    "        }\n",
    "        source.data = copied_data;\n",
    "\n",
    "        const comp_labels = comparison_source.data['comparisons'];\n",
    "        title_obj.text = comp_labels[i];\n",
    "\n",
    "        source.change.emit();\n",
    "    \"\"\")\n",
    "    slider.js_on_change('value', callback)\n",
    "    show(column(slider, plot_emb))\n",
    "\n",
    "    ### can either show or save the plot (cannot do both)\n",
    "    # output_file(\"top_10_tfs_deseq2_adjacent_time_pts_umap_plot.html\")\n",
    "    # save(column(slider, plot_emb))\n",
    "\n",
    "    ### for isolated individual time point images ###\n",
    "    frame_dir = os.path.join(os.getcwd(), image_dir)\n",
    "    os.makedirs(frame_dir, exist_ok=True)\n",
    "    for i, label in enumerate(comparisons):\n",
    "        source.data = dict(sources[i].data)\n",
    "        plot_title.text = label\n",
    "        export_png(plot_emb, filename=os.path.join(frame_dir, f\"frame_{i:02d}_{label}.png\"))\n",
    "\n",
    "    frame_paths = sorted([os.path.join(frame_dir, f) for f in os.listdir(frame_dir) if f.endswith(\".png\")])\n",
    "    images = [Image.open(frame) for frame in frame_paths]\n",
    "    images[0].save(gif_filename, save_all=True, append_images=images[1:], duration=1500, loop=0)\n",
    "    # display(IPyImage(filename=gif_filename))\n",
    "    display(FileLink(gif_filename, result_html_prefix=\"Click here to download: \"))\n",
    "    print(\"CREATED GIF\")\n",
    "\n",
    "    return plot_emb, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed8470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTF-IDF vectorizing gene set data...\n",
      "(699, 1550)\n",
      "\tPerforming Leiden clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewjenkinsvandusen/Downloads/mount_sinai_internship/appyter_310/lib/python3.10/site-packages/scanpy/neighbors/__init__.py:586: UserWarning: Youre trying to run this on 1550 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
      "         Falling back to preprocessing with `sc.pp.pca` and default params.\n",
      "  X = _choose_representation(self._adata, use_rep=use_rep, n_pcs=n_pcs)\n",
      "/var/folders/hs/y48w23_j2gbcvl8vwwd1p3dc0000gn/T/ipykernel_57326/297696729.py:17: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs['leiden'] = 'Cluster ' + adata.obs['leiden'].astype('object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf_time_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 24\u001b[0m\n\u001b[1;32m     12\u001b[0m scatter_df \u001b[38;5;241m=\u001b[39m process_scatterplot(\n\u001b[1;32m     13\u001b[0m     lib_dict,\n\u001b[1;32m     14\u001b[0m     nneighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     15\u001b[0m     mindist\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m legend_description \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlue dots are TFs enriched for upregulated DEGs.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRed dots are TFs enriched for downregulated DEGs.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGreen dots are TFs enriched for both up- and downregulated DEGs.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis study looked at the response of a triple-negative breast cancer cell line to TRAIL.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNA-seq samples were taken at six time points (0, 1, 3, 6, 12, & 24 hours).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m plot_emb, source \u001b[38;5;241m=\u001b[39m get_scatterplot(scatter_df, \u001b[43mtf_time_dict\u001b[49m, comparisons, legend_description, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_10_tfs_deseq2_adjacent_time_pts_umap.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# show(plot_emb)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_time_dict' is not defined"
     ]
    }
   ],
   "source": [
    "r = requests.get(\"https://minio.dev.maayanlab.cloud/hgrn-chear/network_target_sets.gmt\")\n",
    "file = r.text.split(\"\\n\")\n",
    "\n",
    "lib_dict = OrderedDict()\n",
    "for line in file[:-1]:\n",
    "    tokens = line.split(\"\\t\\t\")\n",
    "    term = tokens[0]\n",
    "    genes = [x.split(',')[0].strip() for x in tokens[1].split('\\t')]\n",
    "    lib_dict[term] = ' '.join(genes)\n",
    "\n",
    "## defaults: nneighbors=30, mindist=0.1, spread=1.0, maxdf=1.0, mindf=1\n",
    "scatter_df = process_scatterplot(\n",
    "    lib_dict,\n",
    "    nneighbors=20,\n",
    "    mindist=0.15,\n",
    ")\n",
    "\n",
    "legend_description = (\"Blue dots are TFs enriched for upregulated DEGs.\\n\"\n",
    "    \"Red dots are TFs enriched for downregulated DEGs.\\n\"\n",
    "    \"Green dots are TFs enriched for both up- and downregulated DEGs.\\n\")\n",
    "legend_description += umap_desc\n",
    "\n",
    "if compute_degs:\n",
    "    deseq2_plot_emb_1, deseq2_source_1 = get_scatterplot(scatter_df, deseq2_tf_time_dict_1, deseq2_adj_time_pt_comparisons, legend_description, \"umap_png_frames_deseq2_adjacent_time_pts_top_5_tfs\", \"top_5_tfs_deseq2_adjacent_time_pts_umap.gif\")\n",
    "    deseq2_plot_emb_2, deseq2_source_2 = get_scatterplot(scatter_df, deseq2_tf_time_dict_2, deseq2_time_pt_0_comparisons, legend_description, \"umap_png_frames_deseq2_compare_w_time_pt_0_top_5_tfs\", \"top_5_tfs_deseq2_compare_w_time_pt_0_umap.gif\")\n",
    "\n",
    "    cd_plot_emb_1, cd_source_1 = get_scatterplot(scatter_df, cd_tf_time_dict_1, cd_adj_time_pt_comparisons, legend_description, \"umap_png_frames_cd_adjacent_time_pts_top_5_tfs\", \"top_5_tfs_cd_adjacent_time_pts_umap.gif\")\n",
    "    cd_plot_emb_2, cd_source_2 = get_scatterplot(scatter_df, cd_tf_time_dict_2, cd_time_pt_0_comparisons, legend_description, \"umap_png_frames_cd_compare_w_time_pt_0_top_5_tfs\", \"top_5_tfs_cd_compare_w_time_pt_0_umap.gif\")\n",
    "else:\n",
    "    plot_emb_1, source_1 = get_scatterplot(scatter_df, tf_time_dict_1, comparisons_1, legend_description, \"umap_png_frames_deseq2_adjacent_time_pts_top_5_tfs\", \"top_5_tfs_deseq2_adjacent_time_pts_umap.gif\")\n",
    "    plot_emb_2, source_2 = get_scatterplot(scatter_df, tf_time_dict_2, comparisons_2, legend_description, \"umap_png_frames_deseq2_compare_w_time_pt_0_top_5_tfs\", \"top_5_tfs_deseq2_compare_w_time_pt_0_umap.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appyter_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
